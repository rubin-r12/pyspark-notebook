{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rubinr12/pyspark?scriptVersionId=191676171\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# What is PySpark?\npyspark is a python api for working with apache spark. I will first explain what do I mean by a \"python api\" for something and then explain what, specifically, is 'apache spark'.\n\nwhat I mean by **'python api'** is that you can use the syntex and agility of python to interact with and send commands to a system that is not based, at its core, on python. \n\nwith pyspark, you intercat with apache spark - a system designed for working, analyzing and modeling with immense amounts of data in many computers at the same time. putting it in a different way, apache spark allows you to run computations in parallel, instead of sequentially. it allows you to divide one incredibly large task into many smaller tasks, and run each such task on a different machine.this allowes you to accomplish your analysis goals in reasonable time that would not be possible on a single machine.\n\nusually, we would define the amount of data that suits PySpark as what would not fit into a single machine storage (let alone RAM).\n\n**important related concepts:** \n1. distributed computing - when you distribute a task into several smaller task that run at the same time. this is what pyspark allows you to do with many machines, but it can also be done on a single machine with several threads, for example.\n2. cluster - a network of machines that can take on tasks from a user, interact with one another and return results. these provide the computing resources that pyspark will use to make the computations.\n3. Resilient Distributed Dataset (RDD) - an immutable distributed collection of data. it is not tabular, like DataFrames which we will work with later, and has no data schema. therefore, for tabular data wrangling, DataFrames allowes for more API options and uner-then-hood optimizations. still, you might encounter RDDs as you learn more about Spark, and should be aware of their existence.\n\n**Part of PySpark we will cover:**\n1. PySpark SQL - contains commands for data processing and manipulation.\n2. PySpark MLlib - includes a variety of models, model training and related commands.\n\n**Spark Architecture:**\nto send commands and receive results from a cluster, you will need to initiate a spark session. this object is your tool for interacting with Spark. each user of the cluster will have its own Spark Session, that will allow him to use the cluster in isolation from other users. all of the sessions are communicating with a spark context, which is the master node in the cluster - that is, it assigns each of computers in the cluster tasks and coordinates them. each of the computers in the cluster that perform tasks for a master node is called a worker node. to connect to a worker node, the master node needs to get that node's comput power allocated to it, by a cluster manager, that is responsable for distributing the cluster resources. inside each worker node, there are execute programs that run the tasks - they can run multiple tasks simultaneously, and has their own cashe for storing results. so, each master node can have multiple worker nodes, that can have multiple tasks running.  ","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2024-08-07T17:57:10.960167Z","iopub.execute_input":"2024-08-07T17:57:10.960564Z","iopub.status.idle":"2024-08-07T17:58:06.7861Z","shell.execute_reply.started":"2024-08-07T17:57:10.960533Z","shell.execute_reply":"2024-08-07T17:58:06.784666Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=c3ffec1e60158a661aaedfe639da5cc92f22f92df6911c40c2bc8d5a64d6c483\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# a SparkSession object can perform the most common data processing tasks\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('test').getOrCreate() # will return existing session if one was\n                                                           # created before and was not closed","metadata":{"execution":{"iopub.status.busy":"2024-08-07T17:58:15.584735Z","iopub.execute_input":"2024-08-07T17:58:15.585219Z","iopub.status.idle":"2024-08-07T17:58:21.755077Z","shell.execute_reply.started":"2024-08-07T17:58:15.585181Z","shell.execute_reply":"2024-08-07T17:58:21.75362Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/08/07 17:58:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"spark","metadata":{"execution":{"iopub.status.busy":"2024-08-07T17:58:46.790316Z","iopub.execute_input":"2024-08-07T17:58:46.791123Z","iopub.status.idle":"2024-08-07T17:58:48.061548Z","shell.execute_reply.started":"2024-08-07T17:58:46.791078Z","shell.execute_reply":"2024-08-07T17:58:48.060004Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x7bda5e412ce0>","text/html":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://03e7372b5d55:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>test</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{}}]},{"cell_type":"markdown","source":"**dataset:**\nhttps://www.kaggle.com/fedesoriano/heart-failure-prediction","metadata":{}},{"cell_type":"code","source":"# read csv, all columns will be of type string\n# df = spark.read.option('header','true').csv('heart.csv')\n# tell pyspark the type of the columns - saves time on large dataset. there are other ways to do this, but that's\n# my favorite\nschema = 'Age INTEGER, Sex STRING, ChestPainType STRING'\ndf = spark.read.csv('heart-failure-prediction/heart.csv', schema=schema, header=True)\n# let PySpark infer the schema\ndf = spark.read.csv('heart-failure-prediction/heart.csv', inferSchema=True, header=True)\n# replace nulls with other value at reading time\ndf = spark.read.csv('heart-failure-prediction/heart.csv', nullValue='NA')\n# save data\ndf.write.format(\"csv\").save(\"heart_save.csv\")\n# if you want to overwrite the file\ndf.write.format(\"csv\").mode(\"overwrite\").save(\"heart_save.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T18:06:37.278869Z","iopub.execute_input":"2024-08-07T18:06:37.279364Z","iopub.status.idle":"2024-08-07T18:06:37.433338Z","shell.execute_reply.started":"2024-08-07T18:06:37.279329Z","shell.execute_reply":"2024-08-07T18:06:37.431217Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# read csv, all columns will be of type string\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# df = spark.read.option('header','true').csv('heart.csv')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# tell pyspark the type of the columns - saves time on large dataset. there are other ways to do this, but that's\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# my favorite\u001b[39;00m\n\u001b[1;32m      5\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge INTEGER, Sex STRING, ChestPainType STRING\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheart-failure-prediction/heart.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# let PySpark infer the schema\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheart-failure-prediction/heart.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/kaggle/working/heart-failure-prediction/heart.csv."],"ename":"AnalysisException","evalue":"[PATH_NOT_FOUND] Path does not exist: file:/kaggle/working/heart-failure-prediction/heart.csv.","output_type":"error"}]},{"cell_type":"code","source":"# show head of table\ndf.show(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count number of rows\ndf.count()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show parts of the table\ndf.select('Age').show(3)\ndf.select(['Age','Sex']).show(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pandas DataFrame VS PySpark DataFrame\n\nboth represents a table of data with rows and columns. however, under the hood they are different, as PySpark dataframe needs to support distributed computations. as we move forward, we will see more and more features of it that are not present in Pandas DataFrame. that being said - if you know how to use Pandas, than moving to PySpark will feel like a natural transition.\n\n## DAG\ndirected acyclic graph is the way Spark runs computations. when you give it a series of transformation to apply to the dataset, it build a graph out of those transformations, so it knows what to do - but it does not execute those commands immediately, if it does not have to. rather, it is lazy - it will go through the DAG and apply the transformations only when it must, to provide a needed result. this allows better performance, since spark knows what's ahead of a certain computation and get optimize the process accordingly.\n\n## transformations VS actions\nin PySpark, there are two types of command: transformations and actions. transformation commands are added to the DAG, but does not get it to actually be executed. they transform one DataFrame into another, not changing the input DataFrame. on the other hand, actions make PySpark execute the DAG but does not create a new DataFrame - instead, they output the result of the DAG.\n\n## Caching\nevery time you run a DAG, it will be re-computed from the beginning. that is, the results are not saved in memory. \nso, if we want to save a result so it won't have to be recomputed, we can use the cache command. note, that this will occupy space in the working node's memory - so be careful with the sizes of datasets you are caching! by default, the cached DF is stored to RAM, and is unserialized (not converted into a stream of bytes). you can change both of these - store data to hard disk, serialized it, or both!\n\n## Collecting\neven after caching a DataFrame, it still sits in the worker nodes memory. if you want to collect is pieces, assemble them and save them on the master node so you won't have to pull it every time, use the command for collecting. again, be very careful with this, since the collected file will have to fit in the master node memory!","metadata":{}},{"cell_type":"code","source":"df.cache()\ndf.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert PySpark DataFrame to Pandas DataFrame\npd_df = df.toPandas()\n# convert it back\nspark_df = spark.createDataFrame(pd_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show first three rows as three row objects, which is how spark represents single rows from a table.\n# we will learn more about it later\ndf.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# type os columns\ndf.printSchema()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# column dtypes as list of tuples\ndf.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cast a column from one type to other\nfrom pyspark.sql.types import FloatType\ndf = df.withColumn(\"Age\",df.Age.cast(FloatType()))\ndf = df.withColumn(\"RestingBP\",df.Age.cast(FloatType()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute summery statistics\ndf.select(['Age','RestingBP']).describe().show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add a new column or replace existing one\nAgeFixed = df['Age'] + 1  # select alwayes returns a DataFrame object, and we need a column object\ndf = df.withColumn('AgeFixed', AgeFixed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select(['AgeFixed','Age']).describe().show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove columns\ndf.drop('AgeFixed').show(1) # add df = to get the new DataFrame into a variable","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rename a column\ndf.withColumnRenamed('Age','age').select('age').show(1)\n# to rename more than a single column, i would suggest a loop.\nname_pairs = [('Age','age'),('Sex','sex')]\nfor old_name, new_name in name_pairs:\n    df = df.withColumnRenamed(old_name,new_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select(['age','sex']).show(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop all rows that contain any NA\ndf = df.na.drop()\ndf.count()\n# drop all rows where all values are NA\ndf = df.na.drop(how='all')\n# drop all rows where more at least 2 values are NOT NA\ndf = df.na.drop(thresh=2)\n# drop all rows where any value at specific columns are NAs.\ndf = df.na.drop(how='any', subset=['age','sex']) # 'any' is the defult","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill missing values in a specific column with a '?'\ndf = df.na.fill(value='?',subset=['sex'])\n# replace NAs with mean of column\nfrom pyspark.ml.feature import Imputer # In statistics, imputation is the process of\n                                       # replacing missing data with substituted values\nimptr = Imputer(inputCols=['age','RestingBP'],\n                outputCols=['age','RestingBP']).setStrategy('mean') # can also be 'median' and so on\n\ndf = imptr.fit(df).transform(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filter to adults only and calculate mean\ndf.filter('age > 18')\ndf.where('age > 18')# 'where' is an alias to 'filter'\ndf.where(df['age'] > 18) # third option\n# add another condition ('&' means and, '|' means or)\ndf.where((df['age'] > 18) | (df['ChestPainType'] == 'ATA'))\n# take every record where the 'ChestPainType' is NOT 'ATA'\ndf.filter(~(df['ChestPainType'] == 'ATA'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.filter('age > 18').show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate a string expression into command\nfrom pyspark.sql.functions import expr\nexp = 'age + 0.2 * AgeFixed'\ndf.withColumn('new_col', expr(exp)).select('new_col').show(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# group by age\ndisease_by_age = df.groupby('age').mean().select(['age','avg(HeartDisease)'])\n# sort values in desnding order\nfrom pyspark.sql.functions import desc\ndisease_by_age.orderBy(desc(\"age\")).show(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import asc\ndisease_by_age = df.groupby('age').mean().select(['age','avg(HeartDisease)'])\ndisease_by_age.orderBy(desc(\"age\")).show(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aggregate to get several statistics for several columns\n# the available aggregate functions are avg, max, min, sum, count\nfrom pyspark.sql import functions as F\ndf.agg(F.min(df['age']),F.max(df['age']),F.avg(df['sex'])).show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('HeartDisease').agg(F.min(df['age']),F.avg(df['sex'])).show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run an SQL query on the data\ndf.createOrReplaceTempView(\"df\") # tell PySpark how the table will be called in the SQL query\nspark.sql(\"\"\"SELECT sex from df\"\"\").show(2)\n\n# we also choose columns using SQL sytnx, with a command that combins '.select()' and '.sql()'\ndf.selectExpr(\"age >= 40 as older\", \"age\").show(2)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('age').pivot('sex', (\"M\", \"F\")).count().show(3)››› ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pivot - expensive operation\ndf.selectExpr(\"age >= 40 as older\", \"age\",'sex').groupBy(\"sex\")\\\n                    .pivot(\"older\", (\"true\", \"false\")).count().show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select(['age','MaxHR','Cholesterol']).show(4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# devide dataset to training features and target\nX_column_names = ['Age','Cholesterol']\ntarget_colum_name = ['MaxHR']\n\n# convert feature columns into a columns where the vlues are feature vectors\nfrom pyspark.ml.feature import VectorAssembler\nv_asmblr = VectorAssembler(inputCols=X_column_names, outputCol='Fvec')\ndf = v_asmblr.transform(df)\nX = df.select(['Age','Cholesterol','Fvec','MaxHR'])\nX.show(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# devide dataset into training and testing sets\ntrainset, testset = X.randomSplit([0.8,0.2])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict 'RestingBP' using linear regression\nfrom pyspark.ml.regression import LinearRegression\nmodel = LinearRegression(featuresCol='Fvec', labelCol='MaxHR')\nmodel = model.fit(trainset)\nprint(model.coefficients)\nprint(model.intercept)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model\nmodel.evaluate(testset).predictions.show(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# handel categorical features with ordinal indexing\nfrom pyspark.ml.feature import StringIndexer\nindxr = StringIndexer(inputCol='ChestPainType', outputCol='ChestPainTypeInxed')\nindxr.fit(df).transform(df).select('ChestPainTypeInxed').show(3)","metadata":{},"execution_count":null,"outputs":[]}]}